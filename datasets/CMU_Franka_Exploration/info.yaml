action_space: EEF Position
control_frequency: 10
custom_fields:
  introduction: Human-World Model addresses the challenge of learning complex manipulation skills directly in the real world by leveraging internet-scale human video data, inspired by advancements in computer vision and natural language processing to enable robots to acquire generalizable behaviors from minimal interaction data (as few as 30 minutes per task), with core innovation in a structured human-centric action space based on visual affordances learned from human videos that capture human-object interactions (e.g., grasping, pushing) to provide priors for robotic manipulation; the framework includes an Affordance-Centric World Model pre-trained on diverse human videos to understand object interactions and physical causality (e.g., "a cup can be lifted when grasped") and predict environmental changes from actions, bridging human demonstrations to robot execution, and an Efficient Robot Adaptation component fine-tuned with â‰¤50 robot trajectories per task to ground robot actions in human-derived affordances, reducing exploration complexity and accelerating policy convergence; validated across 7 manipulation tasks (e.g., tool use, deformable object handling), the method achieves task generalization to unseen objects/environments (e.g., kitchen to office), data efficiency with a 90% success rate using 30x less data than RL baselines, and real-world deployability on UR5 and Franka Emika arms in cluttered scenes, unifying human priors with robot learning to pioneer scalable embodied intelligence.
data_collect_method: Expert Policy
depth_cams: 0
episodes: 100
file_size: 2.92
gripper: Default
has_camera_calibration: false
has_proprioception: false
has_suboptimal: true
language_annotations: Templated
license: 'CC BY-NC 4.0'
name: CMU Franka Exploration
rgb_cams: 1
robot: DLR SARA
robot_morphology: Single Arm
scene_type: Kitchen (also toy kitchen)
short_introduction: Human-World Model enables robots to learn manipulation skills in 30 minutes by leveraging visual affordances from human videos. Our affordance-based world model transfers human interaction priors to robots, achieving data-efficient adaptation to complex tasks.
task_description: Franka exploring kitchen environment, lifting knife and vegetable
  and opening cabinet.
url: https://human-world-model.github.io/
wrist_cams: 0
