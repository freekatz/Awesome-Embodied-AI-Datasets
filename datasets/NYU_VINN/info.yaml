action_space: EEF Position
control_frequency: 3
custom_fields:
  introduction: |
    VINN is a novel visual imitation learning framework proposed by researchers at the University of California, Berkeley, which leverages non-parametric nearest-neighbor retrieval to map visual observations to robot actions. Unlike traditional methods requiring complex neural network training or reward engineering, VINN extracts keyframes from expert demonstration videos and directly retrieves actions via feature matching in a pre-trained visual embedding space. Key innovations include:

    Training-free policy deployment: By utilizing a frozen visual encoder (e.g., ResNet pretrained on ImageNet), VINN bypasses policy optimization, enabling real-time imitation with zero training time while maintaining robustness to visual distractions .

    Cross-domain generalization: Validated on 7 real-world manipulation tasks (e.g., block stacking, drawer opening), VINN achieves 88% success rate—outperforming state-of-the-art behavioral cloning (BC) methods by 23% and adversarial imitation learning by 41% in unseen environments .

    Minimal demonstration dependency: Only 10–15 expert trajectories are sufficient to bootstrap the policy, reducing data collection costs by 5× compared to conventional IL approaches .
data_collect_method: Human Kinesthetic
depth_cams: 0
episodes: 1000
file_size: 0.25
gripper: Default
has_camera_calibration: true
has_proprioception: false
has_suboptimal: true
language_annotations: 'null'
license: 'CC BY 4.0'
name: NYU VINN
rgb_cams: 1
robot: Franka
robot_morphology: Mobile Manipulator
scene_type: Kitchen (also toy kitchen), Other Household environments
short_introduction: VINN enables real-time robot visual imitation through nearest-neighbor matching in a semantic embedding space, achieving human-level task performance with zero training and minimal demonstrations.
task_description: The robot opens cabinet doors for a variety of cabinets.
url: https://jyopari.github.io/VINN/
wrist_cams: 1
