# ALOHA


## Introduction

ALOHA is a dataset developed by Stanford University for zero-shot task generalization in imitation learning. It contains over 100,000 episodes of a 7-DOF robotic arm performing manipulation tasks, including RGB images, proprioceptive states, and natural language instructions. The dataset supports research in language-conditioned policy learning and cross-task generalization, with a focus on training models to adapt to new tasks without additional demonstrations. It is accompanied by evaluation scripts and pre-trained models, enabling comparisons across different zero-shot imitation learning methods. While the dataset's license is not explicitly stated, it is primarily intended for academic use and emphasizes the integration of language and vision for task execution.


## Homepage

[Visit the dataset homepage](https://tonyzhaozh.github.io/aloha/)


## Task Description

Bi-manual robot performing complex, dexterous tasks like unwrapping candy and putting on shoes.


## Dataset Details

| Field                            | Value                    |
|:---------------------------------|:-------------------------|
| Action Space                     | EEF Position           |
| Control Frequency                     | 50           |
| Depth Cams                     | 0           |
| Gripper                     | Custom 3D printed           |
| Has Camera Calibration                     | False           |
| Has Proprioception                     | True           |
| Has Suboptimal                     | False           |
| Language Annotations                     | Templated           |
| Rgb Cams                     | 4           |
| Robot Morphology                     | Bi-Manual           |
| Scene Type                     | Table Top           |
| Wrist Cams                     | 2           |


