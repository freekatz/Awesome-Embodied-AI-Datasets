# UCSD Pick Place


## Introduction

UCSD Pick Place is a robotics dataset developed by UC San Diego for vision-based pick-and-place tasks. The dataset contains real-world and simulated trajectories of a UR5 robot manipulating objects in cluttered environments, including RGB images, depth data, and robot joint states. It supports research in open-world model-based reinforcement learning and dynamic manipulation, with a focus on generalization to unseen objects and environments. While the dataset's license is not explicitly stated, it is primarily intended for academic use. UCSD Pick Place is accompanied by evaluation scripts and pre-trained models, enabling comparisons across different vision-based robot learning approaches for pick-and-place tasks.


## Homepage

[Visit the dataset homepage](https://owmcorl.github.io/)


## Task Description

The robot performs pick and place tasks in table top and kitchen scenes. The dataset contains a variety of visual variations.


## Dataset Details

| Field                            | Value                    |
|:---------------------------------|:-------------------------|
| Action Space                     | EEF velocity           |
| Control Frequency                     | 3           |
| Depth Cams                     | 0           |
| Episodes                     | 100           |
| File Size                     |  3.09 GB           |
| Gripper                     | Default           |
| Has Camera Calibration                     | False           |
| Has Proprioception                     | True           |
| Has Suboptimal                     | True           |
| Language Annotations                     | Templated           |
| Rgb Cams                     | 1           |
| Robot                     | DLR EDAN           |
| Robot Morphology                     | Single Arm           |
| Scene Type                     | Table Top, Kitchen (also toy kitchen)           |
| Wrist Cams                     | 0           |


