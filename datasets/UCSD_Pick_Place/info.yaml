action_space: EEF velocity
control_frequency: 3
custom_fields:
  introduction: UCSD Pick Place is a robotics dataset developed by UC San Diego for
    vision-based pick-and-place tasks. The dataset contains real-world and simulated
    trajectories of a UR5 robot manipulating objects in cluttered environments, including
    RGB images, depth data, and robot joint states. It supports research in open-world
    model-based reinforcement learning and dynamic manipulation, with a focus on generalization
    to unseen objects and environments. While the dataset's license is not explicitly
    stated, it is primarily intended for academic use. UCSD Pick Place is accompanied
    by evaluation scripts and pre-trained models, enabling comparisons across different
    vision-based robot learning approaches for pick-and-place tasks.
data_collect_method: Expert Policy
depth_cams: 0
episodes: 100
file_size: 3.09
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: true
language_annotations: Templated
license: 'null'
name: UCSD Pick Place
rgb_cams: 1
robot: DLR EDAN
robot_morphology: Single Arm
scene_type: Table Top, Kitchen (also toy kitchen)
short_introduction: UCSD Pick Place is a UR5 robot dataset for pick-and-place tasks,
  including visual and joint data. It supports open-world RL and dynamic manipulation.
  While the license is unspecified, it emphasizes generalization to unseen objects
  for academic research.
task_description: The robot performs pick and place tasks in table top and kitchen
  scenes. The dataset contains a variety of visual variations.
url: https://owmcorl.github.io/
wrist_cams: 0
