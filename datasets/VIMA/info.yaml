action_space: Primitive skills (pick-place and push)
control_frequency: null due to use of primitive skills
data_collect_method: Scripted
depth_cams: 0
episodes: 'null'
file_size: 'null'
gripper: Suction cup and spatula
has_camera_calibration: true
has_proprioception: false
has_suboptimal: false
introduction: VIMA is a dataset developed by VIMA Labs for vision and language navigation
  tasks. It contains 1,000 episodes of a mobile robot navigating complex environments,
  including RGB images, depth data, and natural language instructions. The dataset
  supports research in open-vocabulary language understanding and real-time robot
  control, with a focus on integrating language and vision for task execution. It
  is accompanied by evaluation scripts and pre-trained models, enabling comparisons
  across different human-robot interaction methods. While the dataset's license is
  not explicitly stated, it is primarily intended for academic use and emphasizes
  the integration of visual and language data for robot navigation tasks.
language_annotations: Multimodal (image + language) templated instructions
license: 'null'
name: VIMA
rgb_cams: 2
robot: 'null'
robot_morphology: Single Arm
scene_type: Table Top
short_description: VIMA is a mobile robot dataset with 1,000 episodes for navigation
  tasks, including visual and language data. It supports open-vocabulary understanding
  and real-time control. While the license is unspecified, it advances human-robot
  interaction research.
task_description: The robot is conditioned on multimodal prompts (mixture of texts,
  images, and video frames) to conduct tabletop manipulation tasks, ranging from rearrangement
  to one-shot imitation.
url: https://vimalabs.github.io/
wrist_cams: 0
