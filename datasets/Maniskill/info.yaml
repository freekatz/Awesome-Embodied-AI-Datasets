action_space: EEF Position
control_frequency: 20
data_collect_method: Scripted
depth_cams: 2
episodes: 9200
file_size: 76.17
gripper: Default
has_camera_calibration: true
has_proprioception: true
has_suboptimal: true
introduction: Maniskill is a large-scale dataset developed by Haoshuai Group for robotic
  manipulation research. It contains over 100,000 episodes of simulated and real-world
  manipulation tasks, including pick-and-place, stacking, and tool use, with RGB images,
  depth data, and robot joint states. The dataset supports research in reinforcement
  learning, imitation learning, and cross-domain generalization, emphasizing the integration
  of vision and proprioception. It is released under the Apache 2.0 license, allowing
  free use and modification for research and development. Maniskill is accompanied
  by a detailed simulation environment and evaluation framework, making it a valuable
  resource for advancing robot learning in complex, dynamic environments.
language_annotations: Templated
license: Apache 2.0
name: Maniskill
rgb_cams: 2
robot: Sawyer
robot_morphology: Single Arm
scene_type: Table Top
short_description: Maniskill is a Haoshuai Group dataset with 100k+ simulated and
  real manipulation episodes, supporting RL and imitation learning. It includes visual
  and proprioceptive data, released under Apache 2.0. It advances cross-domain generalization
  in complex environments.
task_description: The robot interacts with different objects placed on the plane (ground).
  The tasks include picking an isolated object or an object from the clutter up and
  moving it to a goal position, stacking a red cube onto a green cube, inserting a
  peg into the box, assembling kits, plugging a charger into the outlet on the wall,
  turning on a faucet.
url: https://github.com/haosulab/ManiSkill2
wrist_cams: 2
