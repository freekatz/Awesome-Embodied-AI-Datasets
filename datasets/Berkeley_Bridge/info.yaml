action_space: EEF Position
control_frequency: 5
custom_fields:
  introduction: Berkeley Bridge is a large-scale dataset for robot learning, developed
    by UC Berkeley's RAIL Lab. It contains 60,096 trajectories across 24 environments,
    focusing on tasks like pick-and-place, pushing, and folding. The dataset includes
    natural language instructions, multiple camera views, and depth data, supporting
    open-vocabulary and multi-task learning methods. It is designed to facilitate
    research on scalable robot learning by providing extensive task and environment
    variability, enabling models to generalize across domains. Berkeley Bridge is
    released under the MIT license, allowing free use, modification, and distribution.
    The dataset is accompanied by pre-trained models and evaluation scripts, making
    it a valuable resource for accelerating research in imitation learning and offline
    reinforcement learning.
data_collect_method: Human VR
depth_cams: 1
episodes: 150
file_size: 1.33
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: true
language_annotations: Natural
license: 'null'
name: Berkeley Bridge
rgb_cams: 4
robot: xArm
robot_morphology: Single Arm
scene_type: Table Top, Kitchen (also toy kitchen), Other Household environments
short_introduction: Berkeley Bridge is a scalable robot learning dataset with 60,096
  trajectories across 24 environments. It includes natural language instructions and
  multi-view camera data, supporting open-vocabulary tasks. Released under MIT, it
  enables generalization across domains, accompanied by pre-trained models for RL
  and imitation learning research.
task_description: The robot interacts with household environments including kitchens,
  sinks, and tabletops. Skills include object rearrangement, sweeping, stacking, folding,
  and opening/closing doors and drawers.
url: https://rail-berkeley.github.io/bridgedata/
wrist_cams: 1
