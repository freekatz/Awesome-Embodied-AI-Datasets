action_space: EEF Position
control_frequency: 10
custom_fields:
  introduction: |
    The CLVR Jaco Play Dataset is a multimodal robotic interaction dataset curated by researchers at the University of Southern California (USC) and KAIST. It comprises 1,085 teleoperated trajectories collected via Kinova Jaco 2 manipulators in tabletop environments, covering tasks such as object grasping, stacking, and obstacle avoidance. Key features include:

    Multiview Vision: Synchronized RGB streams from third-person (front_cam_ob) and wrist-mounted (mount_cam_ob) cameras;

    Robot States: End-effector Cartesian poses (ee_cartesian_pos_ob), velocities (ee_cartesian_vel_ob), and joint positions (joint_pos_ob);

    Action Annotations: 3D displacement increments + gripper commands ({0: open, 1: hold, 2: close});

    Language Goals: Natural language task descriptions (e.g., "stack the red block on the blue block");

    Reward Signals: Binary success labels per trajectory .

    Sampled at 10Hz, this dataset enables training of language-conditioned policies and imitation learning algorithms for long-horizon manipulation tasks under open-vocabulary instructions.
data_collect_method: Human VR
depth_cams: 0
episodes: 250
file_size: 18.85
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: false
language_annotations: Templated
license: 'null'
name: USC Jaco Play
rgb_cams: 2
robot: Franka
robot_morphology: Single Arm
scene_type: Table Top, Kitchen (also toy kitchen)
short_introduction: CLVR Jaco Play Dataset provides 1,085 language-annotated trajectories from Kinova Jaco 2 robots, featuring synchronized multi-view vision, robot states, and actions for training language-guided manipulation policies.
task_description: The robot performs pick-place tasks in a tabletop toy kitchen environment.
  Some examples of the task include, "Pick up the orange fruit.", "Put the black bowl
  in the sink."
url: https://github.com/clvrai/clvr_jaco_play_dataset
wrist_cams: 1
