action_space: EEF Position
control_frequency: 1
custom_fields:
  introduction: |
    D³Fields is a novel 3D scene representation framework for robotic manipulation, unifying dynamic modeling, semantic understanding, and geometric precision in a zero-shot generalizable paradigm. Its core innovation lies in constructing continuous descriptor fields that encode both geometric properties (e.g., signed distance to object surfaces) and semantic features (e.g., instance masks) by fusing multi-view 2D observations from foundation models (Grounding-DINO, SAM, XMem, DINOv2). Key capabilities include:

    Flexible Goal Specification:

    Enables task definition via 2D images from diverse sources (internet, user photos), establishing dense correspondences between robot workspace and target configurations without retraining .

    Real-time Dynamics Tracking:

    Projects arbitrary 3D points onto RGB-D streams to interpolate fused descriptors, updating object kinematics under deformation or occlusion at 15Hz .

    Zero-Shot Policy Deployment:

    Integrates with Model Predictive Control (MPC) for manipulation planning, achieving 92% success rate in household tasks (e.g., shoe tidying, debris collection) with unseen objects, outperforming Dense Object Nets by 37% in precision .
data_collect_method: Scripted
depth_cams: 4
episodes: 24
file_size: 0.02
gripper: Robotiq 2F-85
has_camera_calibration: false
has_proprioception: true
has_suboptimal: true
language_annotations: 'null'
license: 'MIT'
name: UIUC D3Field
rgb_cams: 4
robot: TidyBot
robot_morphology: Single Arm
scene_type: Table Top
short_introduction: D³Fields enables zero-shot robotic manipulation by fusing multi-view semantic features into dynamic 3D descriptor fields, supporting flexible goal specification via 2D images and real-time kinematics tracking.
task_description: The robot completes tasks specified by the goal image, including
  organizing utensils, shoes, mugs.
url: https://robopil.github.io/d3fields/
wrist_cams: 0
