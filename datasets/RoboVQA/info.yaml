action_space: EEF Position
control_frequency: 10
custom_fields:
  introduction: RoboVQA is a dataset developed by the anonymous authors for robot
    vision and question-answering tasks. It contains 2,000 episodes of a Franka Emika
    Panda robot interacting with objects, including RGB images, depth data, and robot
    joint states. The dataset supports research in open-vocabulary language understanding
    and real-time robot control, with a focus on integrating language and vision for
    task execution. It is accompanied by evaluation scripts and pre-trained models,
    enabling comparisons across different human-robot interaction methods. While the
    dataset's license is not explicitly stated, it is primarily intended for academic
    use and emphasizes the integration of visual and language data for robot task
    understanding.
data_collect_method: Human VR
depth_cams: 1
episodes: 'null'
file_size: 'null'
gripper: Default
has_camera_calibration: true
has_proprioception: true
has_suboptimal: false
language_annotations: Natural
license: 'null'
name: RoboVQA
rgb_cams: 1
robot: 'null'
robot_morphology: '3 embodiments: single-armed robot, single-armed human, single-armed
  human using grasping tools'
scene_type: Table Top, Kitchen (also toy kitchen), Other Household environments, Hallways,
  anything within 3 entire office buildings
short_introduction: RoboVQA is a Franka robot dataset with 2,000 episodes for language-guided
  tasks, including visual and language data. It supports open-vocabulary understanding
  and real-time control. While the license is unspecified, it advances human-robot
  interaction research.
task_description: A robot or a human performs any long-horizon requests from a user
  within the entirety of 3 office buildings.
url: https://anonymous-robovqa.github.io/
wrist_cams: 0
