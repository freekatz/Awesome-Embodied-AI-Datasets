action_space: EEF Position
control_frequency: 10
custom_fields:
  introduction: |
    RobotVQA is a novel visual question answering (VQA) framework designed to bridge scene understanding and robotic action planning. It generates structured scene graphs from RGB-D inputs, enabling robots to interpret complex environments and execute task-oriented queries (e.g., "Which object is graspable on the left shelf?"). Key innovations include:

    Scene graph generation: Converts raw sensor data into semantic graphs with nodes (objects/attributes) and edges (spatial/functional relations), supporting zero-shot transfer from synthetic (VirtualHome) to real-world scenes (72.3% relation prediction accuracy) .

    Actionable VQA: Trains transformer-based models to answer manipulation-focused questions (e.g., "Can the red block be stacked on the blue cylinder?") and output executable action sequences (e.g., [GRASP(red_block), PLACE_ON(blue_cylinder)]) .

    Real-world validation: Deployed on UR5 robots, RobotVQA achieves 89% task success in object retrieval and assembly scenarios by grounding language commands in scene graphs .
data_collect_method: Human VR
depth_cams: 1
episodes: 'null'
file_size: 'null'
gripper: Default
has_camera_calibration: true
has_proprioception: true
has_suboptimal: false
language_annotations: Natural
license: 'BSD 2'
name: RoboVQA
rgb_cams: 1
robot: 'null'
robot_morphology: '3 embodiments: single-armed robot, single-armed human, single-armed
  human using grasping tools'
scene_type: Table Top, Kitchen (also toy kitchen), Other Household environments, Hallways,
  anything within 3 entire office buildings
short_introduction: RobotVQA converts RGB-D inputs into actionable scene graphs for robotic manipulation, enabling language-guided task execution with 89% real-world success.
task_description: A robot or a human performs any long-horizon requests from a user
  within the entirety of 3 office buildings.
url: https://anonymous-robovqa.github.io/
wrist_cams: 0
