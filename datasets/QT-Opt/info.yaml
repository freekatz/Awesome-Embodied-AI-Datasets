action_space: EEF Position
control_frequency: 10
custom_fields:
  introduction: |
    QT-Opt is a scalable deep reinforcement learning framework developed by Google Brain in 2018, designed to enable robots to learn closed-loop vision-based manipulation skills through large-scale real-world data. Its core innovation lies in combining off-policy Q-learning with distributed optimization, allowing robots to continuously adjust grasping strategies based on real-time visual feedback (e.g., RGB camera inputs). Trained on 580,000+ real-world grasp attempts across 1,000 diverse objects, QT-Opt achieves a 96% success rate on unseen objects—outperforming supervised learning baselines by 18% and reducing error rates by 5×. Key advancements include:

    Self-supervised skill emergence: Robots autonomously learn complex behaviors like regrasping, object repositioning, and obstacle displacement without manual programming.

    Cross-domain robustness: Adapts to dynamic disturbances (e.g., human interference) and varying object textures/shapes using only monocular RGB vision.

    Scalable data collection: Leverages distributed training across 7 robots over 4 months, with GPU-accelerated optimization reducing sample complexity by 40%.
data_collect_method: Expert Policy
depth_cams: 0
episodes: 200
file_size: 0.59
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: false
language_annotations: 'null'
license: 'BSD 3'
name: QT-Opt
rgb_cams: 1
robot: Franka
robot_morphology: Single Arm
scene_type: Table Top
short_introduction: QT-Opt is a vision-based robotic manipulation framework that uses distributed deep RL to achieve 96% grasp success on unseen objects, enabling autonomous skill acquisition from 580K+ real-world trials.
task_description: Kuka robot picking objects in a bin.
url: https://arxiv.org/abs/1806.10293
wrist_cams: 0
