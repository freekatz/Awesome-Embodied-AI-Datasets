action_space: Joint position
control_frequency: 30
custom_fields:
  introduction: Robotic Pretrained Transformer (RPT) is a transformer-based model designed to learn unified sensorimotor representations from multimodal robot data by processing interleaved sequences of camera images, proprioceptive robot states, and past actions through tokenizing them into a joint input stream, randomly masking subsets of these tokens during training, and learning to reconstruct the masked content via a masked autoencoding objective to capture cross-modal dependencies crucial for robotic control like correlating visual inputs with motor actions, validated on real-world manipulation tasks as a foundation for efficient policy adaptation with minimal downstream fine-tuning data, featuring key technical innovations including unified tokenization that encodes heterogeneous inputs into a shared token space for seamless multimodal fusion, masked reconstruction that predicts masked tokens via cross-attention over non-masked context to enhance robustness to partial observations, and efficient transfer where pre-trained representations reduce policy training samples by over 40% in downstream tasks such as object grasping and table organization.
data_collect_method: Scripted
depth_cams: 0
episodes: =64*9
file_size: 6.68
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: false
language_annotations: Templated
license: 'CC BY-NC 4.0'
name: Berkeley RPT Data
rgb_cams: 3
robot: Franka
robot_morphology: Single Arm
scene_type: Table Top
short_introduction: RPT (Robotic Pretrained Transformer) learns sensorimotor representations by reconstructing masked tokens from multimodal robot data (images, states, actions), enabling efficient transfer to downstream manipulation policies.
task_description: Picking, stacking, destacking, and bin picking with variations in
  objects.
url: https://arxiv.org/abs/2306.10007
wrist_cams: 1
