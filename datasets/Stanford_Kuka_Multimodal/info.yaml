action_space: EEF Position
control_frequency: 20
custom_fields:
  introduction: |
    The Vision and Touch Dataset is a multimodal robotic interaction dataset developed by researchers at Stanford University, designed to advance self-supervised learning of contact-rich manipulation tasks. It addresses the challenge of combining heterogeneous sensory inputs (vision and touch) for robust control in unstructured environments, where traditional methods struggle due to high-dimensional state spaces and sample inefficiency. Key features include:

    Multimodal Synchronization:

    Combines RGB images (640×480), tactile readings (6-axis force-torque sensor data over 32ms windows), and proprioceptive states (end-effector positions/velocities) to capture interactions during contact-intensive tasks like peg insertion and object assembly.

    Self-Supervised Representation Learning:

    Trains a neural network to fuse vision-touch-proprioception inputs into a compact 128-D latent space via contrastive learning, enabling efficient policy training with 40% fewer samples than standard RL methods.

    Robustness to Perturbations:

    Validated on real-world tasks (e.g., inserting pegs with varying geometries), achieving 92% success under external disturbances (e.g., positional shifts, force noise) and generalizing to unseen object shapes with 85% accuracy.

    This dataset pioneers the use of cross-modal alignment for contact-rich robotics, demonstrating that tactile feedback compensates for visual occlusion while visual context guides tactile interpretation—critical for deformable object manipulation and precision assembly.
data_collect_method: Expert Policy
depth_cams: 0
episodes: 82432
file_size: 799.91
gripper: 'null'
has_camera_calibration: false
has_proprioception: true
has_suboptimal: true
language_annotations: 'null'
license: 'MIT'
name: Stanford Kuka Multimodal
rgb_cams: 1
robot: Multi-Robot
robot_morphology: Single Arm
scene_type: Table Top
short_introduction: The Vision and Touch Dataset enables self-supervised learning of robust multimodal representations for contact-rich robotic tasks, combining synchronized vision, tactile, and proprioception data to improve sample efficiency and generalization.
task_description: The robot learns to insert differently-shaped pegs into differently-shaped
  holes with low tolerances (~2mm).
url: https://sites.google.com/view/visionandtouch
wrist_cams: 0
