action_space: EEF Position
control_frequency: 20
data_collect_method: Expert Policy
depth_cams: 0
episodes: 82432
file_size: 799.91
gripper: 'null'
has_camera_calibration: false
has_proprioception: true
has_suboptimal: true
introduction: Stanford Kuka Multimodal is a dataset developed by Stanford University
  for multimodal robot learning, focusing on contact-rich manipulation tasks. It contains
  3,000 episodes of a Kuka IIWA robot performing peg insertion with force feedback,
  including RGB images, depth data, joint states, and language instructions. The dataset
  supports research in sensor fusion, self-supervised learning, and contact-aware
  control, emphasizing the integration of vision and tactile information. While the
  dataset's license is not explicitly stated, it is primarily intended for academic
  use. Stanford Kuka Multimodal is accompanied by a detailed benchmark and evaluation
  framework, enabling comparisons across different multimodal representation learning
  approaches.
language_annotations: 'null'
license: 'null'
name: Stanford Kuka Multimodal
rgb_cams: 1
robot: Multi-Robot
robot_morphology: Single Arm
scene_type: Table Top
short_description: Stanford Kuka Multimodal is a dataset with 3,000 Kuka robot episodes
  for peg insertion tasks, including visual and force data. It supports sensor fusion
  and contact-aware control. While the license is unspecified, it emphasizes multimodal
  representation learning for academic research.
task_description: The robot learns to insert differently-shaped pegs into differently-shaped
  holes with low tolerances (~2mm).
url: https://sites.google.com/view/visionandtouch
wrist_cams: 0
