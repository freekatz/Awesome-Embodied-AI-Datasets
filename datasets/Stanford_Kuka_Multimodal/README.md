# Stanford Kuka Multimodal


## Introduction

Stanford Kuka Multimodal is a dataset developed by Stanford University for multimodal robot learning, focusing on contact-rich manipulation tasks. It contains 3,000 episodes of a Kuka IIWA robot performing peg insertion with force feedback, including RGB images, depth data, joint states, and language instructions. The dataset supports research in sensor fusion, self-supervised learning, and contact-aware control, emphasizing the integration of vision and tactile information. While the dataset's license is not explicitly stated, it is primarily intended for academic use. Stanford Kuka Multimodal is accompanied by a detailed benchmark and evaluation framework, enabling comparisons across different multimodal representation learning approaches.


## Homepage

[Visit the dataset homepage](https://sites.google.com/view/visionandtouch)


## Task Description

The robot learns to insert differently-shaped pegs into differently-shaped holes with low tolerances (~2mm).


## Dataset Details

| Field                            | Value                    |
|:---------------------------------|:-------------------------|
| Action Space                     | EEF Position           |
| Control Frequency                     | 20           |
| Depth Cams                     | 0           |
| Episodes                     | 82432           |
| File Size                     |  799.91 GB           |
| Has Camera Calibration                     | False           |
| Has Proprioception                     | True           |
| Has Suboptimal                     | True           |
| Rgb Cams                     | 1           |
| Robot                     | Multi-Robot           |
| Robot Morphology                     | Single Arm           |
| Scene Type                     | Table Top           |
| Wrist Cams                     | 0           |


