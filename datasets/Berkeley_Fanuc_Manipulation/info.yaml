action_space: EEF Position
control_frequency: 10
custom_fields:
  introduction: The FANUC Manipulation Dataset is a comprehensive vision-based robotic learning resource collected via a FANUC Mate 200iD robot, integrating multi-view visual data (third-person fixed and egocentric wrist-mounted 224Ã—224 RGB videos), robot trajectory data (joint positions, gripper states, velocities, and Cartesian-space actions), and language task instructions (natural language commands for conditioning manipulation tasks), designed to advance imitation learning and vision-motor policy training by enabling visuomotor policy development (training closed-loop control models from visual inputs to actions), visual representation fine-tuning (adapting pre-trained models like ResNet and ViT to robotic domains), and instruction-conditioned generation (learning policies mapping language commands to robot behaviors), with a structured format supporting seamless integration with modern frameworks such as PyTorch and TensorFlow to facilitate cross-modal learning and sim-to-real transfer research.
data_collect_method: Human VR
depth_cams: 0
episodes: 233000
file_size: 765
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: true
language_annotations: Natural
license: 'MIT'
name: Berkeley Fanuc Manipulation
rgb_cams: 2
robot: Hello Stretch
robot_morphology: Single Arm
scene_type: Table Top
short_introduction: The FANUC Manipulation Dataset offers multi-view video, proprioceptive data, and language instructions from a FANUC Mate 200iD robot. It enables training visuomotor policies, fine-tuning vision models, and instruction-conditioned action generation for robotic manipulation.
task_description: A Fanuc robot performs various manipulation tasks. For example,
  it opens drawers, picks up objects, closes doors, closes computers, and pushes objects
  to desired locations.
url: https://sites.google.com/berkeley.edu/fanuc-manipulation
wrist_cams: 1
