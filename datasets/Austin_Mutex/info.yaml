action_space: EEF Position
control_frequency: 20
custom_fields:
  introduction: The MUTEX (Multi-Object Tracking Under Exocentric View) dataset is a specialized
    benchmark for multi-object tracking in exocentric perspectives, such as aerial views from drones
    or elevated cameras. Developed by the Robot Perception Lab at UT Austin, it features diverse real-world
    scenarios with complex object interactions, occlusions, and environmental challenges. The dataset provides
    high-precision bounding box annotations and trajectory IDs across varied lighting/weather conditions,
    specifically designed to advance research in occlusion handling and viewpoint-invariant tracking algorithms. 
    Licensed under CC BY-NC 4.0, it supports non-commercial academic use with mandatory attribution.
data_collect_method: Human Spacemouse
depth_cams: 0
episodes: 660103
file_size: 1390
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: false
language_annotations: Natural Language annotations generate with GPT4 and followed
  by human correction.
license: 'MIT'
name: Austin Mutex
rgb_cams: 2
robot: UR5
robot_morphology: Single Arm
scene_type: Table Top
short_introduction: MUTEX is an exocentric-view multi-object tracking dataset featuring challenging real-world 
  scenarios with precise annotations, designed to advance occlusion-robust tracking research.


task_description: The Mutex dataset involves a diverse range of tasks in a home environment,
  encompassing pick and place tasks like "putting bread on a plate," as well as contact-rich
  tasks such as "opening an air fryer and putting a bowl with dogs in it" or "taking
  out a tray from the oven and placing bread on it."
url: https://ut-austin-rpl.github.io/MUTEX/
wrist_cams: 1
