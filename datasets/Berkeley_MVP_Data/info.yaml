action_space: Joint position
control_frequency: 5
custom_fields:
  introduction: This repository offers the official PyTorch implementation for two groundbreaking papers on masked visual pretraining in robot learning—"Masked Visual Pre-training for Motor Control" and "Real-World Robot Learning via Masked Visual Pre-training"—which tackle the critical challenge of obtaining generalizable visual representations from limited task-specific data in vision-based robotics; inspired by the success of masked autoencoders (MAEs) in computer vision, the proposed unified pretraining framework reconstructs masked robot observations (such as RGB images and proprioceptive states) to learn transferable spatiotemporal features that accelerate downstream policy learning for motor control tasks and decrease reliance on large-scale in-domain demonstrations, with core technical components including Masked Robot Modeling (MRM), a self-supervised objective for cross-modal representation learning by reconstructing randomly masked patches from robot camera streams and sensor readings; efficient policy adaptation using pretrained weights to initialize visuomotor policies (PPO/BC), reducing downstream training samples by over 50% while enhancing sim-to-real transfer robustness; and benchmark validation showing state-of-the-art performance on 7 real-world tasks like door opening and pick-and-place, achieving a 63% average success rate improvement over non-pretrained baselines, and the repository includes pretrained vision models (ResNet-18/50, ViT-Base) on robotics datasets such as BridgeData V2, modular PPO and BC training code for policy fine-tuning, and evaluation scripts for simulated and physical deployments.
data_collect_method: Human VR
depth_cams: 0
episodes: 4200
file_size: 720
gripper: Default
has_camera_calibration: false
has_proprioception: true
has_suboptimal: false
language_annotations: Templated
license: 'CC BY-NC 4.0'
name: Berkeley MVP Data
rgb_cams: 1
robot: Franka
robot_morphology: Single Arm
scene_type: Table Top, Kitchen (also toy kitchen)
short_introduction: Masked Visual Pretraining for Robot Learning offers PyTorch implementations for self-supervised representation learning on robot data. Pretrained models enable efficient adaptation of PPO/BC policies, validated on real-world tasks like manipulation and navigation.
task_description: Basic motor control tasks (reach, push, pick) on table top and toy
  environments (toy kitchen, toy fridge).
url: https://arxiv.org/abs/2203.06173
wrist_cams: 1
